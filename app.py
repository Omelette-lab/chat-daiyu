import gradio as gr
import lm_api
import sys
import os
from dataclasses import asdict, dataclass
import time
import asyncio  

now_dir = os.getcwd()
sys.path.append(now_dir)
sys.path.append("%s/SoVits" % (now_dir))
from api import handle
import logging  

model_lm, tokenizer_lm = lm_api.load_model()
max_length_lm=2048
top_p_lm=0.75
temperature_lm=0.5
generation_config_lm = lm_api.prepare_generation_config(max_length_lm,top_p_lm,temperature_lm) 

user_prompt = '<|im_start|>user\n{user}<|im_end|>\n'
robot_prompt = '<|im_start|>assistant\n{robot}<|im_end|>\n'
cur_query_prompt = '<|im_start|>user\n{user}<|im_end|>\n\
    <|im_start|>assistant\n'

theme = 'ParityError/Anime'



# é…ç½®æ—¥å¿—è®°å½•å™¨  
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')  
  

async def txt_to_audio_async(text):
    return handle(text)
# éŸ³é¢‘æ¨¡å—

async def txt_to_audio(text):  
    return await txt_to_audio_async(text)
      


async def daiyu_chat(question: str, chat_history: list = None):
    logging.info("å˜é‡ question çš„å€¼ä¸º: ",question)
    logging.info("å˜é‡ question çš„ç±»å‹ä¸º: ",type(question))
    if question == None or len(question) < 1:
        return "", chat_history
    try:
        real_prompt = lm_api.combine_history(question,chat_history)
        result = ""
        # éå† generate_interactive å‡½æ•°ç”Ÿæˆçš„å“åº”ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­  
        for cur_response in lm_api.generate_interactive(  
            model=model_lm,  
            tokenizer=tokenizer_lm,  
            prompt= real_prompt,  
            additional_eos_token_id=92542,  
            **asdict(generation_config_lm),  
        ):  
            result = cur_response
        logging.info("result: ",result)
        ans = result
        audio_file = await txt_to_audio(ans)
        # logging.info("ç­‰å¾…30s.....")
        # time.sleep(15)
        logging.info("audio_file: ",audio_file)
        # èŠå¤©å‡½æ•°
        chat_history.append(
            (question, ans)
        )
        return "", chat_history ,audio_file
    except Exception as e:
        return e, chat_history

def daiyu_message(question: str, chat_history: list = None):
    # question = "ä½ æ˜¯è°ï¼Ÿ"
    logging.info("å˜é‡ question çš„å€¼ä¸º: ",question)
    logging.info("å˜é‡ question çš„ç±»å‹ä¸º: ",type(question))
    if question == None or len(question) < 1:
        return "", chat_history
    try:
        real_prompt = lm_api.combine_history(question,chat_history)
        result = ""
        # éå† generate_interactive å‡½æ•°ç”Ÿæˆçš„å“åº”ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­  
        for cur_response in lm_api.generate_interactive(  
            model=model_lm,  
            tokenizer=tokenizer_lm,  
            prompt= real_prompt,  
            additional_eos_token_id=92542,  
            **asdict(generation_config_lm),  
        ):  
            result = cur_response
        logging.info("result: ",result)
        ans = result
        # èŠå¤©å‡½æ•°
        chat_history.append(
            (question, ans)
        )
        return "", chat_history 
    except Exception as e:
        return e, chat_history



block_1 = gr.Blocks()
with block_1 as demo_1:
    with gr.Row(equal_height=True):
        with gr.Column(scale=15):
            gr.Markdown(
                """
                <h1>
                <center>Chat-daiyu</center>
                </h1>
                <center>â€œæ•…äººæ˜¯è°ï¼Ÿâ€â€œå§‘è‹æ—é»›ç‰ã€‚â€</center>
                """)

    with gr.Row(equal_height=True):
        with gr.Column(scale=2):
            with gr.Row():
                audiobot = gr.Audio(
                    type="filepath",
                    interactive=False,
                    autoplay=True
                )
            with gr.Row():
                gr.Image(
                    value="/root/chat-daiyu/src/daiyu.jpg",
                    interactive=False,
                    height="auto",
                    label="daiyu",
                    type="pil"
                )

            with gr.Row():
                max_length_lm = gr.Slider(0, 32768, value=8192, step=1.0, label="Maximum length", interactive=True)
                top_p_lm = gr.Slider(0, 1, value=0.8, step=0.01,label="Top P", interactive=True)
                temperature_lm = gr.Slider(0, 1.5, value=0.95, step=0.01, label="Temperature", interactive=True)

        with gr.Column(scale=6):
            chatbot = gr.Chatbot(height=695, show_copy_button=True)
        

    with gr.Row(equal_height=True):
        with gr.Column(scale=8):
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
            msg = gr.Textbox(label="åœ¨æ­¤è¾“å…¥èŠå¤©å†…å®¹...")
            with gr.Row():
                # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                submit_btn = gr.Button("å‘é€")
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(
                    components=[chatbot], value="æ¸…é™¤èŠå¤©è®°å½•")
            

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        submit_btn.click(
            fn=daiyu_chat,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot,audiobot],
        )








        # submit_btn.click(txt_to_audio,
        #                inputs=[chatbot],
        #                outputs=[audiobot])



#     # gr.Markdown("""ğŸä¸èµ«èé—²èŠæ—¶çš„æç¤ºğŸï¼š
#     # <br>
#     # 1. ğŸ¯è¯­éŸ³ç‰ˆå› ä¸ºç®—åŠ›é™åˆ¶ï¼Œè¿ç®—æ—¶é—´è¾ƒé•¿(>=20s, <=100s)ï¼Œè¯·è€å¿ƒç­‰å¾…ğŸ¯
#     # 2. âœ¨å¦‚æœå¸Œæœ›èƒ½å¤Ÿä¸è´¤ç‹¼èµ«èå¿«é€Ÿæ²Ÿé€šï¼Œå»ºè®®ä½¿ç”¨ Chatty-Chatty ç‰ˆæœ¬(å·¦ä¸Šè§’ Tab)âœ¨
#     # 3. ğŸŒ ç‰ˆæœ¬è™½ç„¶æœ‰ä¸€å®šé²æ£’æ€§ï¼Œä½†æ˜¯é™äºä¸ªäººæŠ€æœ¯ï¼Œè¯·å°½å¯èƒ½ä½¿ç”¨ä¸­æ–‡ä¸”å‡å°‘é”™å­—ğŸŒ 
#     # 4. ğŸ†•è¯¥ç‰ˆæœ¬ä¸‹ï¼Œæ¨¡å‹å¯¹é—®é¢˜çš„å›ç­”ä¼šè½¬åŒ–ä¸ºéŸ³é¢‘ï¼Œæ”¾ç½®äºéŸ³é¢‘è¾“å‡ºæ¡†å†…ğŸ†•
#     # 5. ğŸŸç”±äºé¡¹ç›®æŠ€æœ¯å®ç°éƒ¨åˆ†æ˜¯æˆ‘ä¸ªäººç‹¬è‡ªè´Ÿè´£ï¼Œæ‰€ä»¥æ¨¡å‹å¯èƒ½ä¼šå‡ºç°ä¸€äº›å°é—®é¢˜ï¼Œæ„Ÿè°¢æ‚¨çš„æŒ‡æ­£ğŸŸ
#     # <br>
#     # """)

# threads to consume the request
gr.close_all()

# æ„å»º gradio å¯¹è¯
block_2 = gr.Blocks()
with block_2 as demo_2:
    with gr.Row(equal_height=True):
        with gr.Column(scale=15):
            gr.Markdown(
                """
                <h1>
                <center>Chat-daiyuç®€æ´ç‰ˆ</center>
                </h1>
                <center>â€œæ•…äººæ˜¯è°ï¼Ÿâ€â€œå§‘è‹æ—é»›ç‰ã€‚â€</center>
                """)

    with gr.Row(equal_height=True):
        with gr.Column(scale=6):
            chatbot = gr.Chatbot(height=450, show_copy_button=True)

    with gr.Row(equal_height=True):
        with gr.Column(scale=8):
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
            msg = gr.Textbox(label="åœ¨æ­¤è¾“å…¥èŠå¤©å†…å®¹...")
            with gr.Row():
                # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                submit_btn = gr.Button("å‘é€")
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(
                    components=[chatbot], value="Clear console")

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        submit_btn.click(
            fn=daiyu_message,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot],
        )

# #     gr.Markdown("""ğŸä¸èµ«èé—²èŠæ—¶çš„æç¤ºğŸï¼š
# #     <br>
# #     1. ğŸ¯è¯­éŸ³ç‰ˆå› ä¸ºç®—åŠ›é™åˆ¶ï¼Œè¿ç®—æ—¶é—´è¾ƒé•¿(>=20s, <=100s)ï¼Œè¯·è€å¿ƒç­‰å¾…ğŸ¯
# #     2. âœ¨å¦‚æœå¸Œæœ›èƒ½å¤Ÿä¸è´¤ç‹¼èµ«èå¿«é€Ÿæ²Ÿé€šï¼Œå»ºè®®ä½¿ç”¨ Chatty-Chatty ç‰ˆæœ¬(å·¦ä¸Šè§’ Tab)âœ¨
# #     3. ğŸŒ ç‰ˆæœ¬è™½ç„¶æœ‰ä¸€å®šé²æ£’æ€§ï¼Œä½†æ˜¯é™äºä¸ªäººæŠ€æœ¯ï¼Œè¯·å°½å¯èƒ½ä½¿ç”¨ä¸­æ–‡ä¸”å‡å°‘é”™å­—ğŸŒ 
# #     4. ğŸŸç”±äºé¡¹ç›®æŠ€æœ¯å®ç°æ˜¯æˆ‘ä¸ªäººç‹¬è‡ªè´Ÿè´£ï¼Œæ‰€ä»¥æ¨¡å‹å®ç°ä¼šå‡ºç°ä¸€äº›å°é—®é¢˜ï¼Œæ„Ÿè°¢æ‚¨çš„æŒ‡æ­£ğŸŸ
# #     <br>
# #     """)

# threads to consume the request
demo = gr.TabbedInterface(
    [block_1, block_2],
    # ["Voicy_Voicy", "Chatty_Chatty"],
    # [block_1],
    ["Chat-daiyu","Chat-daiyuç®€æ´ç‰ˆ"],
    theme=theme
)
# threads to consume the request
gr.close_all()
# # é’ˆå¯¹ Gradioçš„ç¾åŒ–

# å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
# demo.launch(share=True, server_port=int(os.environ['PORT1']))
# ç›´æ¥å¯åŠ¨
demo.launch(share=True, server_port=7860)

